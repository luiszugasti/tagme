package ru.eb02;

/**
 * This sample Main Run completes the full testing process for improving the provided
 * search results, by building a document graph.
 * The steps are detailed as follows:
 *
 * =========================OPENING OF RELEVANT DATA=========================
 *
 * 1. First, the Tagme Config service is initated. What this does is take the defined configuration
 * xml file and applies the configuration settings to a TAGME instance. In our specific case, we
 * have configured our environment for high Java heap space, and run the TAGME service in FAST mode.
 * This Tagme initiation is completed by TagmeConfig.init() and the generation of Relatedness
 * measures, topic searchers, tagme parsers, disambugator, segmentation, and the rho measure
 * objects, respectively.
 *
 * 2. Next, for a clean run, we open two provided files:
 * -  The first file is the TREC results as provided in an original run of the baseline search
 *    method. This should be taken care of by a call to specific FileTools methods.
 * -  The second file is the TREC hit list as generated by the original baseline method. This file
 *    will contain the document names required to build specific graphs for 200 independent
 *    queries.
 * What the result of this creates is the baseline TREC topics, and the baseline results.
 *
 * 2.a) with the baseline TREC results, these results are saved into a monolithic trecResult
 * object that encapsulates all scores. This baseline score set will act as a reference for results
 * in the following steps.
 *
 * 2.b) with the baseline topics, we gather all the documents required by these baselines and
 * process them through a simple pipeline, as defined in the Doc class. This is achieved by
 * running the document through Jsoup and then running the stripped text through a stop word
 * remover. These documents are then saved in a large Document Hash Map.
 * The objective of this document Hash Map: to keep the documents linked quickly by their String
 * name and a reference to their Doc object instance. This then allows for fast read only access
 * of documents' top edges (a method to be run at the instantiation of a document itself).
 * Finally, this allows us to add the documents to a trecTopic.
 *
 * =========================GENERATION OF LIVE DATA=========================
 *
 * 3. Once we have all the related topics completed, we are able to generate document graphs
 * for each of the topics. These document graphs are created via the following process:
 *
 * 3.a) All the document titles are added as vertices to the graph. This is done using the addVertex
 * method.
 *
 * 3.b) Using a document's topMap, all the edges are computed for the documents via the
 * addEdge method, which checks for duplicate edges and assumes all vertices provided to it are
 * already added. It's important to note that the topMap does not include all documents' top
 * entities, however, this can easily be modified for subsequent runs.
 *
 * Of course, the above two steps are repeated for all the document graphs, and DocGraphs live
 * alongside trecTopic objects. The essential point is that DocGraphs are reusable, while trecTopics
 * are semi reusable only for testing purposes. The way that this is achieved is in the
 * This allows us to perform testing in as an efficient way as possible
 * (to the best of my knowledge).
 *
 * =========================TESTING VIA GRID SEARCH METHODS=========================
 *
 * Up to this point, we have the following key data structures:
 * -  trecTopics, up to 200 of them, which always keep the original scores and rankings of the
 *    baseline search method. Additionally, are linked to a single query.
 * -  Document graph for each query, up to 200 of them. These document graphs have a method called
 *    computeCentrality(double threshold) which will compute centrality of a graph, after
 *    removing edges below the provided threshold.
 * -  Baseline trecResult object, containing all the original values of MAP, Precision, and more.
 *
 * These individual data structures do not help much without some automation, via the use of grid
 * search. Now, I follow with some final discussion into the testing phase, which encompasses
 * mainly functional/non-functional requirements for the Grid Search implementation.
 *
 * 4. The grid search method is encapsulated as a static method within algorithms. It accepts the
 * following inputs:
 *
 * -  double[] lambdaSteps: This contains all the values to test for lambda.
 * -  double[] centralitySteps: This contains all the steps for the cutoff values of centrality.
 * -  int kill: (Optional) This value, when set to a positive value, denotes the steps between
 *    a local maximum in grid search and ending the whole run.
 * -  trecTopic[] topics: the complete set of original topics for this run. Array size is assumed
 *    unbounded.
 * -  DocGraph[] docGraphs: an array of document graphs to run simultaneous centrality experiments.
 * -  trecResult baseTrecResult: the base trecResult list to compare improvement measures upon.
 *
 * 4.a) PERFORMANCE REQUIREMENTS: The Grid Search method will be expected to run each Grid Search
 * test sequentially. However, each Grid Search test will be composed of sub runs which should be
 * encapsulated in the LinkedParameters inner class. This will keep track of current values for a
 * run and run tests in parallel, independently, for 200 queries. It is expected to use as many
 * parallel cores as possible.
 *      Once all results are obtained, the resulting document orderings are sent to the TREC eval
 * executable by an external OS call. The top 20 documents being improved are all we care about.
 * The value of improvement is considered and saved to a variable which will allow tracking of
 * subsequent test improvements. If kill is not used, then this variable will track the ID of the
 * best run (these runs will all be saved to the file system with their file names identifying
 * the values of their variables, in hopes of identifying concavity of the solution space).
 *
 * 4.b) MEMORY REQUIREMENTS: It is expected that at any one time, only a single test run of
 * centrality will be running; hence, it is not necessary at this stage to keep the representations
 * of documents. These will be removed before running grid search.
 *
 * that is all I can think of. Future luis: keep strong, and keep testing code for a successful run!
 *
 * ==================================================
 *
 *
 *
 */
import it.acubelab.tagme.Disambiguator;
import it.acubelab.tagme.RelatednessMeasure;
import it.acubelab.tagme.RhoMeasure;
import it.acubelab.tagme.Segmentation;
import it.acubelab.tagme.TagmeParser;
import it.acubelab.tagme.config.TagmeConfig;
import it.acubelab.tagme.preprocessing.TopicSearcher;
import java.io.File;
import java.io.IOException;
import java.util.HashMap;

/**
 * (Currently) a sequentially based, entity finder for documents.
 */
public class Main {

  public static void main(String[] args) throws IOException {

    // Initiates the TAGME service with the configuration defined by me.
    TagmeConfig.init();
    String wikiLanguage = "en";
    RelatednessMeasure rel = RelatednessMeasure.create(wikiLanguage);
    TopicSearcher searcher = new TopicSearcher(wikiLanguage);
    TagmeParser parser = new TagmeParser(wikiLanguage, true);
    Disambiguator disamb = new Disambiguator(wikiLanguage);
    Segmentation segmentation = new Segmentation();
    RhoMeasure rho = new RhoMeasure();

    // TODO: hardcoded directory!
    String documentDir = "/home/luis-zugasti/EB02 Document Corpus/clueweb09PoolFilesTest";
    // I wanna use ArrayList but my hand is forced with HashMap.
    // And turns out I'll just leave HashMap as is... whatever I figured out a way to prevent
    // duplicate edges that doesn't use this.
    HashMap<Doc, Integer> mapDocs = new HashMap<>();


    // Batch processing: Get all the entities.
    File folder = new File(documentDir);
    File[] docNameList = folder.listFiles();
    for(File file : docNameList) {
      if (file.isFile()) {
        String tmpFilePath = documentDir + "/" + file.getName();

        // docs will have their file names and stopword stripped text, added here.
        Doc temp = new Doc(FileTools.readFileUTF8(tmpFilePath, false), file.getName(),
            wikiLanguage, rel, disamb, segmentation, rho, parser);

        // docs will then have their entities added here.
        temp.obtainEntities(wikiLanguage, rel, disamb, segmentation, rho, parser);
        mapDocs.put(temp, 1);
      }
    }

    // Batch processing: Compare each document to the other and build the graph.
    // ONE IS A STUB.
    DocGraph finalGraph = new DocGraph(0.5, 1);
    for (Doc documentA : mapDocs.keySet()) {
      for (Doc documentB : mapDocs.keySet()) {
        if (finalGraph.duplicateEdgeCheck(documentA.getDocName(), documentB.getDocName()) ||
            (documentA.getDocName()).equals(documentB.getDocName())) {
          // we are dealing with the same document OR about to add a duplicate edge,
          // which will raise an exception. Skip.
          break;
        }
//        finalGraph.addTuple(documentA.getTopMap(), documentB.getTopMap(), documentA.getDocName(),
//            documentB.getDocName(), rel);
      }
    }
  }
}
